---
title: "As-you-go" instead of "after-the-fact": A network approach to scholarly communication and evaluation" 
bibliography: Meta-science.bib
csl: mdpi.csl
abstract: ""
keywords: ""
output: 
  word_document:
    reference_docx: mdpi-formatting.docx
---

**Abstract:** Scholarly research faces severe threats to its sustainability on multiple domains (access, incentives, reproducibility, inclusivity). We argue that "after-the-fact" research papers do not help and actually cause some of these threats because the chronology of the research cycle is lost in a research paper. We propose to give up the academic paper and propose a digitally native "as-you-go" alternative. In this design, smaller pieces of research outputs are communicated along the way and are linked to each other to form a network of outputs that can facilitate research evaluation. This embeds chronology in the design of scholarly communication and facilitates recognition of more diverse outputs that go beyond the paper (e.g., code, materials). Moreover, using network analysis to investigate the relations between linked outputs could help align evaluation tools with evaluation questions. We illustrate how such an "as-you-go" design of scholarly communication could be structured and how network metrics could be computed to assist in the evaluation process, with specific use cases for funders, universities, and individual researchers.

**Keywords:** evaluation; network; communication; paper; metaresearch; decentralization; decentralisation; publishing

#1. Introduction

  Scholarly research faces severe threats to its sustainability and has been said to face a reproducibility crisis [@Baker2016] amongst other pernicious problems such as access and exclusivity. The underlying cause might be the way we have collectively designed the reporting and rewarding of  research (implicitly or explicitly). The current scholarly communication system is organized around researchers who publish papers in journals. Many of these journals have artificial page limits (in the digital age), which leads to artificial scarcity and subsequently increases the perceived prestige of such a journal due to high rejection rates. Furthermore, scholarly communication has become highly centralized, where over 50% of all papers are published by as little as five publishers (over 70% for social sciences) [@Lariviere2015]. Centralization has introduced knowledge discrimination, as publishers are able to influence who can access scholarly knowledge, what gets published, and allows for other single points of failure to arise with their own consequences (e.g., censorship; https://perma.cc/HDX8-DJ8F). In order to have a sustainable scholarly research system, severe changes are necessary that provide a more coherent answer to all threats instead of addressing them individually.

  The design of a system directly affects what the system and the people who use it can do; scholarly communication still retains an analog based design affecting the effectivity of the spread and production of knowledge dissemination (its goal). Researchers and institutions are evaluated on where and how much papers they publish (as a form of prestige). For example, an oft-used measure of quality is the Journal Impact Factor (JIF) [@Garfield2006]. The JIF is also frequently used to evaluate the 'quality' of individual papers under the assumption that a high impact factor predicts the success of individual articles (this assumption has been debunked many times) [@Prathap2016; @Seglen1992; @Seglen1994]. Many other performance metrics in the current system (e.g., citation counts and h-indices) resort to generic bean counting. Inadequate evaluation measures leave universities, individual researchers, and funders (amongst others) in the dark with respect to the substantive questions they might have about the produced scholarly knowledge. Additionally, work that is not aptly captured by papers receives less recognition despite potential value (e.g., software code). It is impossible that a paper-based approach to scholarly communication can escape the consequences of paper's limitations.	

  We propose an alternative design for scholarly communication based on piecemeal research outputs with direct links between subsequent outputs, forming a network. Whereas a paper-based approach communicates after a whole research cycle is completed, we propose to  communicate piecemeal parts of the research cycle on an "as-you-go" basis. These pieces could be similar to sections of a research paper, but extend to things such as software or materials. An "as-you-go" communication design respects the chronological nature of research cycles and decreases the possibility for pernicious problems such as selective publication and making predictions after results are known (HARKing) [@Kerr1998].

  With a network structure between piecemeal outputs of knowledge, we can go beyond citations and facilitate different questions about single- or collectives of knowledge. For example, how central is a single output in the larger network? Or: How densely interconnected is this collective of knowledge outputs? A network could facilitate question-driven evaluation where a metric needs to be operationalized per question, instead of metrics that have become a goal in themselves and become invalidated by clear cheating behaviors [@Seeber2017; @PLOSMedicine2006]. As such, we propose to make evaluation of research its own research process with question formulation, operationalizations, and data collection (i.e., constructing the network of interest). 

#2. Network structure

  Research outputs are typically papers, which report on at least one research cycle after it has occurred. The communicative design of papers embeds hindsight and its biases in the reporting of results. Moreover, this design eliminates the verification of the chronology within a paper. On the other hand, the paper encompasses so much that citations to other papers can indicate a tangent or a crucial link. Additionally, the paper is a bottleneck for what is communicated: It cannot properly deal with code, data, materials, etc.

  When stages of research are communicated separately and as they occur, it changes the communicative design to eliminate hindsight and allows more types of outputs to be communicated. For example, a theory can be communicated first and hypotheses communicated afterwards, as a direct descendant of the theory. Subsequently, a design can be linked as a direct descendant of the hypotheses, materials as a direct descendant of the design, and so on. This would allow for the incorporation of materials, data, and analysis code (amongst others). In this structure, many nodes could link to a single node (e.g., replication causes many data nodes to connect to the same hypotheses node) but one node can also link to many other nodes (e.g., when hypotheses follow from multiple theories or when a meta-analytic node is linked to many results nodes).

  Figure 1 shows a simple example of how these different research outputs (i.e., nodes) would directly connect to each other. The connection between these nodes only shows the direct descendance and could still include citations to other pieces of information. For example, a discussion section could be a direct descendant of a results section and could still include citations to other relevant findings. When one research cycle ends, a new one can link to the last node, continuing the chain of descendance.

![**Figure 1. A Directed Acyclic Graph (DAG) of connected research stages. The ordering is chronological (top-bottom) and therefore nodes that are situated below one another cannot refer upwards.**](C:/Users/s379011/Dropbox/projects/2017network-rankings/docs/Topological_Ordering.png)

  Given that these piecemeal outputs would be communicated as they occur, chronology is directly embedded in the communication process with many added benefits. For example, preregistration of hypotheses tries to ensure that predictions precede observations, which would be embedded with piecemeal communication where predictions are communicated when they are made. Moreover, if research outputs are communicated as they are produced, selective reporting (i.e., publication bias) is reduced by having already communicated the data before results are generated. 

  With immutable append-only registers, the chronology and content integrity of these outputs can be ensured and preserved over time. This can occur efficiently and elegantly with the Dat protocol. In short, the Dat protocol is a peer-to-peer protocol (i.e., decentralized and openly accessible) that provides non-adjustable timestamps to each change that occurs within a folder, which is given a permanent unique address on the peer-to-peer Web (36^64 addresses possible) [@Dat2017]. The full details, implications, and potential implementations of this protocol for scholarly communication fall outside of the scope of this paper.

#3. Metrics

  With a chronological ordering of various research outputs and their parent relations, a directional adjacency matrix can be extracted for network analysis. Table 1 shows the directional adjacency matrix for Figure 1. Parent nodes must precede the child nodes in time, therefore only $\frac{J(J-1)}{2}$ of cells of the adjacency matrix are filled in, where $J$ is the number of research outputs.

**Table 1. Directional adjacency matrix for Figure 1. Nodes are ordered according to time (top-bottom in Figure 1). Rows indicate the source node, columns indicate the target node.**

|    	| node01 | node02 | node03 | node04 | node05 | node06 | node07 | node08 | node09 |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| node01 | -  	| 1  	| 0  	| 1  	| 0  	| 0  	| 0  	| 0  	| 0  	|
| node02 | -  	| -  	| 1  	| 0  	| 0  	| 0  	| 1  	| 0  	| 0  	|
| node03 | -  	| -  	| -  	| 0  	| 0  	| 0  	| 0  	| 0  	| 0  	|
| node04 | -  	| -  	| -  	| -  	| 1  	| 1  	| 0  	| 0  	| 1  	|
| node05 | -  	| -  	| -  	| -  	| -  	| 0  	| 0  	| 1  	| 1  	|
| node06 | -  	| -  	| -  	| -  	| -  	| -  	| 1  	| 0  	| 0  	|
| node07 | -  	| -  	| -  	| -  	| -  	| -  	| -  	| 1  	| 0  	|
| node08 | -  	| -  	| -  	| -  	| -  	| -  	| -  	| -  	| 0  	|
| node09 | -  	| -  	| -  	| -  	| -  	| -  	| -  	| -  	| -  	|

  With a directional adjacency matrix, countless network metrics can be calculated that could be useful in research evaluation depending on the questions asked. However, not all network metrics are directly applicable because a time based component is included in the network (i.e., new outputs cannot refer to even newer outputs). Below, we propose some basic network metrics for evaluating past and future research outputs.

  Networks metrics could be used to evaluate the network as it exists now or developed in the past (i.e., backward-looking evaluation). For example, in-degree centrality, a measure indicating how many child nodes are spawned by a parent node can be used as a measure to quantify how much work a researcher's output stimulates new knowledge producing efforts (e.g., node04 in Table 1 would have an in-degree centrality of three). To contextualize this, an example could be that a data node spawns four results sections, hence has an in-degree centrality of four. This measure would look only at one-generation of child nodes, but other measures extend this to incorporate multiple generations of child nodes (e.g., 'Katz centrality') [@Wasserman1994] (pp. 206-210). For example, two data nodes that each spawn five results nodes would have the same in-degree centrality, but could have different Katz centrality if only one of those two networks has a third-generation of nodes included. 

  Another set of network metrics could be used to evaluate how the network would change when new nodes are added in the future (i.e., forward-looking evaluation). For example, a researcher who is looking for ways to increase the density in their own network, could ask the question "If I would add one node that has $N$ parents, which addition would increase the density the most?" Subsequently, the researcher could inspect the identified connections for inspiration and feasibility. Complexity of the new node could be increased by increasing the number of parent nodes to connect ($N$ in the question; e.g., five instead of two). Potentially, this could facilitate creative thinking, where $N$ is gradually increased over time to increase the complexity of the issue from a network perspective.

#4. Use cases

  We describe three use cases of network based evaluation to contextualize the ideas proposed above. For each use case, we first provide a general and non-exhaustive overview of the possibilities with network based evaluation. Subsequently, we specify a scenario for that use case, how an evaluation question flows from that scenario, how a metric to answer that question could be operationalized, and how that metric could inform the evaluation process. With these use cases we hope to illustrate that network based evaluation would align better with the implicit evaluation criteria already present in common research evaluation scenarios.
  
##Funders

  Funders of scholarly research have certain aims when distributing their financial resources amongst researchers. Currently, funders often use generic "one size fits all" metrics to evaluate the quality of researchers (e.g., JIF, h-index, citation counts). Given that funders often have specific aims with funding calls, these funding calls could form the basis of research evaluation. Potentially, given that network analysis allows different questions to be asked, funders might also change the aims of funding calls to shift towards more specific goals than generic notions of "innovation" or "discovery".

  One specific scenario is that of a funding agency calling for proposals to fund researchers to extend an already existing research line. This is not an implausible scenario, where funding agencies aim to fund several million dollars (or similar in other currencies) in order to increase follow through in research lines. A specific example might be the Dutch national funding agency "Vici" funding scheme, which aims to fund "senior researchers who have successfully demonstrated the ability to develop their own innovative lines of research" (https://perma.cc/GB83-RE4J).

  Whether researchers who submitted proposals actually have built a research line could be evaluated by looking at how interconnected each researcher's personal network is. Let us assume that a research line here would mean that new research efforts interconnect with previous efforts by that same researcher (i.e., building on previous work). Additionally, we could assume that building a research line means that the research line becomes more present in the network over the years. Building a research line thus could be reformulated into questions about the network of directly linked output and its development over time.

  Operationalizing the concept research line as increased interconnectedness of outputs over time, we could compute the network density of an applicant over the last five years to inform how the applicant's research aligns with the aim of the funding scheme. One way of computing density would be to tally the number of links and divide them by the number of possible links. By taking snapshots of the network of outputs of that researcher in the last five years on January 1st, we could compute a metric to inform us about the development of the researcher's network of outputs.

  The development of network density over time could help inform the evaluation, but one measure could hardly be deemed the only decision criterion. As such, it only provides an indication as to whether an applicant aligns with the aim of the funding agency. Other questions still need to be answered by the committee. For example, is the project feasible or does the proposal extend the previous research line? Some of these other questions could also be seen as questions about the future development of the network and investigated, supplying a set of metrics with which to evaluate the applicant. 

##Universities

  Universities use research evaluation for the internal allocation of resources and to hire new scientists. A research group within a university could use network analysis to assess how (dis)connected a group's outputs are or how their group compares to similar groups at other institutions.  Using network metrics, it would also become possible to assess whether a job applicant fulfills certain criteria, such as whether their outputs connect to existing outputs of a group. If a university wants to stimulate more diversity in research background, network analysis could also be used to identify those who are farther removed from the current researchers at the institution.  Considering that universities are often evaluated on the same generic metrics as individual researchers (e.g., JIF) in the rankings, such new and more precise evaluation tools might also help specify goals of a university and reduce the rat race. Like the use case for funders, network based evaluation allows universities to focus on question- and mission aligned evaluation. 

  Extending the scenario above, imagine a research group that is looking to hire an assistant professor with the aim of increasing connectivity between the group's members. The head of the research group made this her personal goal in order to facilitate more information exchange and collaborative potential within the group. By making increasing connectivity within the group an explicit aim of the hiring process, it can be incorporated into the evaluation process. 

  In order to achieve the increased connectivity within the research group, the head of the research group wants to evaluate applicants relatively but also with an absolute standard. Relative evaluation facilitates applicant selection, but absolute evaluation facilitates whether any applicant achieves the goal to begin with. In other words, relative evaluation here asks which is the best applicant, whereas absolute evaluation asks whether the best applicant is is good enough.

  Increased connectivity can be computed as a difference measure of the research group's network density with and without the applicant. In order to take into account the number of produced outputs, the computed density takes into account the number of outputs of an applicant. Moreover, the head stipulates that the minimum increase in network density needs to be five percentage points. To evaluate applicants, each gets a score that is made up of the difference between the current network density and the network density if they were hired. For example, baseline connectivity within a group might be 60%, hence, the network density has to be at least 65% for one of the applicants to pass the evaluation criterium. 

  If the head of the research group relied purely on an increase in network density metric without further evaluation, a hire that decreases morale in the research group could easily be made. For example, it is reasonable to assume that critics of a research group often link research outputs in a criticism of their work. If such a person would apply for a job within that group, the density within the network might be increased but subsequently result in a more hostile work climate. Without evaluating the content of the applicant that increases the network density, it is hard to assess whether they would actually increase information exchange and collaborative potential instead of stifling it.

##Individuals

  Individual researchers can use networks to better understand their research outputs and plan new research efforts. For example, simply visualizing a network of outputs could already prove a useful tool for researchers to view relationships between their outputs from a different, more coherent perspective. Researchers looking for new research opportunities could also use network analysis to identify their strengths, by comparing whether specific sets of outputs are more central than others in a larger network. For example, a researcher who writes software for their research might find that their software is more central in a larger network than their findings, which could indicate a fruitful specialization. 

  One scenario where network evaluation tools could be valuable for individual researchers is in order to optimize resource allocation. A researcher might want to revisit previous work and conduct a large replication, but only has funds for one replication. Imagine a researcher who wants to identify an effect previously studied that has been central to their new research efforts. Identifying which effect to replicate is intended by this researcher as a safeguard mechanism to prevent further investment in new studies, if a fundamental finding proves to not be replicable.

  In this scenario, the researcher aims to identify the most central finding in a network. The researcher has conducted many studies throughout their career and does not want to identify the most central finding in the entire network of outputs over the years, but only of the most recent domain they've been working in. As such, the researcher takes the latest output and traces all the preceding outputs automatically, to create a subset of the full network and to incorporate potential work not done by themselves. 

  Subsequently, by computing the Katz centrality of the resulting subnetwork, the researcher can compute the number of outputs generated by a finding and how many outputs those outputs generated in return. By assigning this value to each node in the network, the researcher can identify the most central nodes. However, these nodes need to be investigated subsequently in order to see whether they are findings or something else (e.g., theory; we assume an agnostic infrastructure that does not classify nodes).

  Centrality can be a useful measure to identify which finding to replicate, but would fail to take into account what replications have already been conducted. When taking the most recent output and looking at its parent(s), grandparent(s), etc., this only looks at the lineage of the finding. However, the children of all these parents are not taken into account in such a trace. As such, the researcher in our scenario might identify an important piece of research to replicate, but neglect that it has already been replicated. Without further inspection of the network for already available replications, resource allocation might be suboptimal after all.

#5. Discussion

  We propose to communicate research in piecemeal "as-you-go" units (e.g., theory followed by hypotheses, etc.) instead of large "after-the-fact" papers. Piecemeal communication opens up the possibility of a network of knowledge to come into existence when these pieces are linked (e.g., results descend from data). This network of knowledge would be supplementary to traditional citation networks and could facilitate new evaluation tools that are based in the question of interest than generic "one size fits all" evaluation metrics (e.g., Journal Impact Factor, citation counts, number of publications). Given the countless questions and operationalizations possible to evaluate research in a network of knowledge, we hope this would increase the focus on metrics as a tool in the evaluation process instead of metrics being the evaluation process itself [@Hicks2015]. 

  We highlighted a few use cases and potential metrics for funders, research collectives, and individuals, but recognize that we are merely scratching the surface of possible use cases and implementations of network analysis in research evaluation. Award committees might use critical path analysis or network stability analysis to identify key hubs in a network to recognize. Moreover, services could be built to harness the information available in a network to identify people who could be approached for collaborations or to facilitate the ease with which such network analyses can be conducted. Future work could investigate more use cases, qualitatively identify what researchers (or others) would like to know from such networks, and how existing network analysis methods could be harnessed to evaluate research and better understand its development over time. Despite our enthusiasm for network based evaluation, we also recognize the need for exploring the potential negative sides of it. Proximity effects might increase bias towards people already embedded in a network and might exacerbate inequalities already present.

  Communicating scholarly research in smaller "as-you-go" units might also address other threats to research sustainability. In piecemeal "as-you-go" communication, selective publication based on results would be reduced because data would be communicated before results are known. Similarly, adjusting predictions after results are known would be reduced because predictions would be communicated before data are available (i.e., preregistration by design). Replications (or reanalyses) would be encouraged both for the replicated (the replicated node gets more child nodes, increasing its centrality) and the replicator (time investment is lower due to only having to add a data node that is linked to the materials node of the replicated). Self-plagiarism could be reduced by not forcing researchers to rehash the same theory across papers that spawn various predictions and studies. These various issues (amongst other out of scope issues) could be addressed jointly instead of each issue vying for importance for researchers, funders, or policy makers (amongst others). 

  To encourage culture- and behavioral change, "after-the-fact" papers and piecemeal "as-you-go" outputs could co-exist (initially) and would not require researchers to make a zero-sum decision. Copyright is often transferred to publishers upon publication (resulting in pay-to-access), but only after a legal contract is signed. Hence, preprints cannot be legally restricted by publishers when they precede a copyright transfer agreement. However, preprints face institutional and social opposition [@Kaiser2017], where preprinting could exclude a manuscript for publication depending on editorial policies or due to fears of non-publication or scooping (itself a result of hypercompetition). In recent years, preprints have become more widely accepted and less likely to exclude manuscript publication (e.g., Science accepts preprinted manuscripts) [@Berg2017]. Similarly, sharing piecemeal "as-you-go" outputs would not be legally restricted by publishers and could ride the wave of preprint acceptance, but might also face institutional or social counterchange similar to the history of preprints. Researchers could communicate "as-they-go" and compile "after-the-fact" papers, facilitating co-existence and minimizing negative effects on career opportunities.

  As far as we know, piecemeal "as-you-go" scholarly communication infrastructure has not yet been available to researchers in a sustainable way. The only thought style that has facilitated "as-you-go" reporting in the past decade is that of Open Notebook Science (ONS) [@Bradley2007], where researchers share their day-to-day notes and thoughts. However, ONS has remained on the fringes of the Open Science thought style and has not matured, limiting its usefulness and uptake. For example, ONS increases user control because communication occurs on personal domains, but does not have a mechanism of preserving the content. Considering reference rot occurs in seven out of ten scholarly papers containing Weblinks [@Klein2014], concern for sustainable ONS is warranted without further development of content integrity. Moreover, ONS increases information output without providing more possibilities of discovering that content.

  Digital infrastructure that facilitates "as-you-go" scholarly communication is now feasible and sustainable. Feasible because the peer-to-peer protocol Dat provides stable addresses for versioned content and it ensures content integrity across those versions. Sustainable because preservation in a peer-to-peer network is relatively trivial (inherent redundancy, anyone can rehost information and libraries could be persistent hosters) and removes (or at least reduces) the need for centralized services in scholarly communication. Consequently, this decreases the need for inefficient server farms of centralized services [@Cavdar2012] by decentralizing services. Another form of sustainability is knowledge inclusion, which is facilitated by a decentralized network protocol that is openly accessible.

  Finally, we would like to note that communication was not instantly revolutionized by the printing press but changed society over the centuries that followed. The Web has only been around since 1991 and its effect on society is already pervasive, but far from over. We hope that individuals who want change do not despair by feelings of inertia in scholarly communication throughout recent years and further entrenching of positions and interests. We remain optimistic for substantial change to occur within scholarly communication that improves the way we communicate research and hope these ideas contribute in working towards that. 

#References