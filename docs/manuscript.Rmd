---
title: "As-you-go" instead of "after-the-fact": A network approach to scholarly communication and evaluation" 
bibliography: Meta-science.bib
csl: mdpi.csl
abstract: ""
keywords: ""
output: 
  word_document:
    reference_docx: mdpi-formatting.docx
---

**Abstract:** Scholarly research faces threats to its sustainability on multiple domains (access, incentives, reproducibility, inclusivity). We argue that "after-the-fact" research papers do not help and actually cause some of these threats because the chronology of the research cycle is lost in a research paper. We propose to give up the academic paper and propose a digitally native "as-you-go" alternative. In this design, smaller pieces of research outputs are communicated along the way and are linked to each other to form a network of outputs that can facilitate research evaluation. This embeds chronology in the design of scholarly communication and facilitates recognition of more diverse outputs that go beyond the paper (e.g., code, materials). Moreover, using network analysis to investigate the relations between linked outputs could help align evaluation tools with evaluation questions. We illustrate how such an "as-you-go" design of scholarly communication could be structured and how network indicators could be computed to assist in the evaluation process, with specific use cases for funders, universities, and individual researchers.

**Keywords:** evaluation; network; communication; paper; metaresearch; decentralization; decentralisation; publishing

#1. Introduction

  Scholarly research faces threats to its sustainability and has been said to face a reproducibility crisis [@Baker2016] amongst other pernicious problems such as access and exclusivity. The underlying cause might be the way we have collectively designed the reporting and rewarding of research (implicitly or explicitly). The current scholarly communication system is primarily organized around researchers who publish static (digital) research papers in scholarly journals. Many of these journals have artificial page limits (in the digital age), which leads to artificial scarcity and subsequently increases the perceived prestige of such a journal due to high rejection rates (71% on average for APA journals in 2016; [perma.cc/Q7AT-RN5C](https://perma.cc/Q7AT-RN5C)). Furthermore, scholarly communication has become highly centralized, where over 50% of all papers are published by as little as five publishers (over 70% for social sciences) [@Lariviere2015]. Centralization has introduced knowledge discrimination, as publishers are able to influence who can access scholarly knowledge, what gets published, and allows for other single points of failure to arise with their own consequences (e.g., censorship; [perma.cc/HDX8-DJ8F](https://perma.cc/HDX8-DJ8F)). In order to have a sustainable scholarly research system, we consider it necessary to implement changes that provide progress on multiple of these threats at once instead of addressing them individually.

  Systems design directly affects what the system and the people who use it can do; scholarly communication still retains an analog based design affecting the effectivity of the spread and production of knowledge dissemination (see also [@Kling2005]). Researchers and institutions are evaluated on where and how much papers they publish (as a form of prestige). For example, an oft-used measure of quality is the Journal Impact Factor (JIF) [@Garfield2006]. The JIF is also frequently used to evaluate the 'quality' of individual papers under the assumption that a high impact factor predicts the success of individual papers (this assumption has been debunked many times) [@Prathap2016; @Seglen1992; @Seglen1994]. Many other performance indicators in the current system (e.g., citation counts and h-indices) resort to generic bean counting. Inadequate evaluation measures leave universities, individual researchers, and funders (amongst others) in the dark with respect to the substantive questions they might have about the produced scholarly knowledge. Additionally, work that is not aptly captured by the authorship of papers is likely to receive less recognition (e.g., writing software code) due to reward systems counting publications instead of contributions (see also [perma.cc/MUH7-VCA9](https://perma.cc/MUH7-VCA9)). It is unfeasible that a paper-based approach to scholarly communication can escape the consequences of paper's limitations.	

  A scholarly communication system is supposed to serve five functions, but can do so in a narrow sense as it currently does, or in a wider sense. These functions of the scholarly communication system are (1) registration-, (2) certification-, (3) awareness-, and (4) archival [@roosendaal1997], and (5) incentives [@VandeSompel2004]. A narrow fulfillment of for example the registration function would mean that findings that are published are registered, but not all findings are registered (e.g., due to selective publication; [@10.1126/science.1255484]). Similarly, certification is supposed to occur through peer review, but peer review can exacerbate human biases in the assessment of quality (e.g., statistical significance increasing the perceived quality of methods; [@10.1007/bf01173636]).

  We propose an alternative design for scholarly communication based on modular research outputs with direct links between subsequent modules, forming a network. Whereas a paper-based approach communicates after a whole research cycle is completed, modular communication was proposed two decades ago [@Kircz1998;@VandeSompel2004;@10.7717/peerj-cs.78;@10.3233/ISU-2010-0613;@10.1163/095796511X560006]. These modules could be similar to sections of a research paper, but extend to modular research outputs such as software or materials. We propose to implement this modular communication on an "as-you-go" basis and include direct links to indicate provenance. This respects the chronological nature of research cycles and decreases the possibility for pernicious problems such as selective publication and making predictions after results are known (HARKing) [@Kerr1998].

  With a network structure between modules of knowledge, we can go beyond citations and facilitate different questions about single- or collectives of knowledge. For example, how central is a single module in the larger network? Or: How densely interconnected is this collective of knowledge modules? A network could facilitate question-driven evaluation where an indicator needs to be operationalized per question, instead of indicators that have become a goal in themselves and become invalidated by clear cheating behaviors [@Seeber2017; @PLOSMedicine2006]. As such, we propose to make evaluation of research its own research process with question formulation, operationalizations, and data collection (i.e., constructing the network of interest). 

#2. Network structure

  Research outputs are typically research papers, which report on at least one research cycle after it has occurred. The communicative design of papers embeds hindsight and its biases in the reporting of results by being inherently reconstructive. Moreover, this design eliminates the verification of the chronology within a paper. On the other hand, the paper encompasses so much that citations to other papers can indicate a tangent or a crucial link. Additionally, the paper is a bottleneck for what is communicated: It cannot properly deal with code, data, materials, etc.

  When stages of research are communicated separately and as they occur, it changes the communicative design to eliminate hindsight and allows more types of outputs to be communicated as separate modules. For example, a theory can be communicated first and hypotheses communicated second, as a direct descendant of the theory. Subsequently, a study design can be linked as a direct descendant of the hypotheses, materials as a direct descendant of the design, and so on. This would allow for the incorporation of materials, data, and analysis code (amongst others). In this structure, many modules could link to a single module (e.g., replication causes many data modules to connect to the same hypotheses module) but one module can also link to many other modules (e.g., when hypotheses follow from multiple theories or when a meta-analytic node is linked to many results nodes).

  Figure 1 shows a simple example of how these different modular research outputs (i.e., nodes) would directly connect to each other. The connection between these nodes only shows the direct descendance and could still include citations to other pieces of information. For example, a discussion node could be a direct descendant of a results node and could still include citations to other relevant findings. When one research cycle ends, a new one can link to the last node, continuing the chain of descendance. Incorporating the direct descendancy of these knowledge modules builds a different kind of network than citation and authorship networks. As such, this network would be an addition to these already existing citation and authorship networks; it does not seek to replace them.

![**Figure 1. Two Directed Acyclic Graphs (DAGs) of connected research stages. The ordering is chronological (top-bottom) and therefore nodes that are situated below one another cannot refer upwards. Panel A shows a less complex network of modules; Panel B shows a more extensive network of modules.**](Topological_Ordering.png)

  Given that these modular outputs would be communicated as they occur, chronology is directly embedded in the communication process with many added benefits. For example, preregistration of hypotheses tries to ensure that predictions precede observations, which would be embedded with modular communication where predictions are communicated when they are made [exploratory research could be communicated without hypotheses; for a more extensive discussion of the benefits and limits of preregistration see @10.1073/pnas.1708274114]. Moreover, if modular outputs are communicated as they are produced, selective reporting (i.e., publication bias) is reduced by having already communicated the data before results are generated. 

  With immutable append-only registers, the chronology and content integrity of these outputs can be ensured and preserved over time. This can occur efficiently and elegantly with the Dat protocol (without a blockchain; [perma.cc/GC8X-VQ4K](https://perma.cc/GC8X-VQ4K)). In short, the Dat protocol is a peer-to-peer protocol (i.e., decentralized and openly accessible) that provides non-adjustable timestamps to each change that occurs within a folder, which is given a permanent unique address on the peer-to-peer Web (36^64 addresses possible) [@Dat2017]. The full details, implications, and potential implementations of this protocol for scholarly communication fall outside of the scope of this paper (see [https://dat-com-chris.hashbase.io](https://web.archive.org/web/20180316145316/https://dat-com-chris.hashbase.io/)).

  A continuous and network based communication system could take a wider interpretation of the scholarly functions it is supposed to serve [@roosendaal1997; @10.1045/september2004-vandesompel]. Registration would become more complete, because selective publication based on results is preempted by embedding communication before any results are known. Certification is improved by embedding the chronology of a research cycle into the communication of research, ensuring that predictions precede results [@10.1073/pnas.1708274114]. Awareness is improved by using open by design principles, whereas awareness is now limited by financial means to access scholarly papers [@10.12688/f1000research.12037.3]. Archival would not only be simplified with peer-to-peer protocols, but also allows anyone to create a copy and could result in excessive redundancy under the Lots Of Copies Keeps Stuff Safe principle [LOCKSS; @10.1045/june2001-reich]. In the next sections, we extend on how incentives could be adjusted in such a network structure, to facilitate both the evaluation of research(ers) and the planning of research.

#3. Indicators

  With a chronological ordering of various modular research outputs and their parent relations, a directional adjacency matrix can be extracted for network analysis. Table 1 shows the directional adjacency matrix for Figure 1. Parent nodes (i.e., modules) must precede the child nodes in time, therefore only $\frac{J(J-1)}{2}$ of cells of the adjacency matrix are filled in, where $J$ is the number of research modules.

**Table 1. Directional adjacency matrix for Figure 1. Nodes are ordered according to time (top-bottom in Figure 1). Rows indicate the source node, columns indicate the target node.**

|    	| node01 | node02 | node03 | node04 | node05 | node06 | node07 | node08 | node09 |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| node01 | -  	| 1  	| 0  	| 1  	| 0  	| 0  	| 0  	| 0  	| 0  	|
| node02 | -  	| -  	| 1  	| 0  	| 0  	| 0  	| 1  	| 0  	| 0  	|
| node03 | -  	| -  	| -  	| 0  	| 0  	| 0  	| 0  	| 0  	| 0  	|
| node04 | -  	| -  	| -  	| -  	| 1  	| 1  	| 0  	| 0  	| 1  	|
| node05 | -  	| -  	| -  	| -  	| -  	| 0  	| 0  	| 1  	| 1  	|
| node06 | -  	| -  	| -  	| -  	| -  	| -  	| 1  	| 0  	| 0  	|
| node07 | -  	| -  	| -  	| -  	| -  	| -  	| -  	| 1  	| 0  	|
| node08 | -  	| -  	| -  	| -  	| -  	| -  	| -  	| -  	| 0  	|
| node09 | -  	| -  	| -  	| -  	| -  	| -  	| -  	| -  	| -  	|

  With a directional adjacency matrix, countless network indicators can be calculated that could be useful in research evaluation depending on the questions asked. However, not all network indicators are directly applicable because a time based component is included in the network (i.e., new outputs cannot refer to even newer outputs). Below, we propose some basic network indicators for evaluating past and future research outputs.

  Networks indicators could be used to evaluate the network as it exists now or how it developed in the past (i.e., backward-looking evaluation). For example, in-degree centrality could be used to identify highly interconnected modules of information. This measure indicates how many child nodes are spawned by a parent node and indicates how much new work a researcher's output stimulates (e.g., node04 in Table 1 would have an in-degree centrality of three). To contextualize this, a data node could spawn four results sections, hence has an in-degree centrality of four. This measure would look only at one-generation of child nodes, but other measures extend this to incorporate multiple generations of child nodes. Katz centrality extends this and computes the centrality over $N$ generations of child nodes [pp. 206-210; @Wasserman1994] whereas traditional in-degree centrality calculates centrality for $N=1$ generations. For example, two data nodes that each spawn five results nodes would have the same in-degree centrality, but could have different Katz centrality if only one of those two networks has a third-generation of nodes included. If multi-generation indicators are relevant, Katz centrality measures could provide operationalizations of such measures.

  Another set of network indicators could be used to evaluate how the network would change when new nodes are added in the future (i.e., forward-looking evaluation). For example, a researcher who is looking for ways to increase the density in their own network, could ask the question "If I would add one node that has $N$ parents, which addition would increase the density the most?" Subsequently, the researcher could inspect the identified connections for inspiration and feasibility. Complexity of the new node could be increased by increasing the number of parent nodes to connect ($N$ in the question; e.g., five instead of two). Potentially, this could facilitate creative thinking, where $N$ is gradually increased over time to increase the complexity of the issue from a network perspective.

  The indicators we highlighted here are simple proposals. Indicators from network analysis and graph theory could be applied to the study of knowledge development when a network structure is available and we hope to see suggestions to answer questions about the network. These kinds of analyses are already done within citation networks [e.g., @10.1126/science.aao0185] and authorship networks [e.g., @10.1371/journal.pntd.0000501], but we cannot do so with the provenance or planning of knowledge generation in the current scholarly communication system.

#4. Use cases

  We describe three use cases of network based evaluation to contextualize the ideas proposed above. For each use case, we first provide a general and non-exhaustive overview of the possibilities with network based evaluation. Subsequently, we specify a scenario for that use case, how an evaluation question flows from that scenario, how an indicator to answer that question could be operationalized, and how that indicator could inform the evaluation process. With these use cases we hope to illustrate that network based evaluation would align better with the implicit evaluation criteria already present in common research evaluation scenarios.
  
##Funders

  Funders of scholarly research have certain aims when distributing their financial resources amongst researchers. Currently, funders often use generic "one size fits all" indicators to evaluate the quality of researchers (e.g., JIF, h-index, citation counts). Given that funders often have specific aims with funding calls, these funding calls could form the basis of research evaluation. Potentially, given that network analysis allows different questions to be asked, funders might also change the aims of funding calls to shift towards more specific goals than generic notions of "innovation" or "discovery".

  One specific scenario is that of a funding agency calling for proposals to fund researchers to extend an already existing research line. This is not an implausible scenario, where funding agencies aim to fund several million dollars (or similar in other currencies) in order to increase follow through in research lines. A specific example might be the Dutch national funding agency "Vici" funding scheme, which aims to fund "senior researchers who have successfully demonstrated the ability to develop their own innovative lines of research" (https://perma.cc/GB83-RE4J).

  Whether researchers who submitted proposals actually have built a research line could be evaluated by looking at how interconnected each researcher's personal network is. Let us assume that a research line here would mean that new research efforts interconnect with previous efforts by that same researcher (i.e., building on previous work). Additionally, we could assume that building a research line means that the research line becomes more present in the network over the years. Building a research line thus could be reformulated into questions about the network of directly linked output and its development over time.

  Operationalizing the concept research line as increased interconnectedness of outputs over time, we could compute the network density of an applicant over the last five years to inform how the applicant's research aligns with the aim of the funding scheme. One way of computing density would be to tally the number of links and divide them by the number of possible links. By taking snapshots of the network of outputs of that researcher in the last five years on January 1st, we could compute an indicator to inform us about the development of the researcher's network of outputs.

  The development of network density over time could help inform the evaluation, but one measure could hardly be deemed the only decision criterion. As such, it only provides an indication as to whether an applicant aligns with the aim of the funding agency. Other questions still need to be answered by the committee. For example, is the project feasible or does the proposal extend the previous research line? Some of these other questions could also be seen as questions about the future development of the network and investigated, supplying a set of indicators with which to evaluate the applicant. 

##Universities

  Universities use research evaluation for the internal allocation of resources and to hire new scientists. A research group within a university could use network analysis to assess how (dis)connected a group's outputs are or how their group compares to similar groups at other institutions.  Using network indicators, it would also become possible to assess whether a job applicant fulfills certain criteria, such as whether their outputs connect to existing outputs of a group. If a university wants to stimulate more diversity in research background, network analysis could also be used to identify those who are farther removed from the current researchers at the institution.  Considering that universities are often evaluated on the same generic indicators as individual researchers (e.g., JIF) in the rankings, such new and more precise evaluation tools might also help specify goals of a university and reduce the rat race. Like the use case for funders, network based evaluation allows universities to focus on question- and mission aligned evaluation. 

  Extending the scenario above, imagine a research group that is looking to hire an assistant professor with the aim of increasing connectivity between the group's members. The head of the research group made this her personal goal in order to facilitate more information exchange and collaborative potential within the group. By making increasing connectivity within the group an explicit aim of the hiring process, it can be incorporated into the evaluation process. 

  In order to achieve the increased connectivity within the research group, the head of the research group wants to evaluate applicants relatively but also with an absolute standard. Relative evaluation facilitates applicant selection, but absolute evaluation facilitates whether any applicant achieves the goal to begin with. In other words, relative evaluation here asks which is the best applicant, whereas absolute evaluation asks whether the best applicant is is good enough.

  Increased connectivity can be computed as a difference measure of the research group's network density with and without the applicant. In order to take into account the number of produced outputs, the computed density takes into account the number of outputs of an applicant. Moreover, the head stipulates that the minimum increase in network density needs to be five percentage points. To evaluate applicants, each gets a score that is made up of the difference between the current network density and the network density if they were hired. For example, baseline connectivity within a group might be 60%, hence, the network density has to be at least 65% for one of the applicants to pass the evaluation criterium. 

  If the head of the research group relied purely on an increase in network density indicator without further evaluation, a hire that decreases morale in the research group could easily be made. For example, it is reasonable to assume that critics of a research group often link research outputs in a criticism of their work. If such a person would apply for a job within that group, the density within the network might be increased but subsequently result in a more hostile work climate. Without evaluating the content of the applicant that increases the network density, it is hard to assess whether they would actually increase information exchange and collaborative potential instead of stifling it.

##Individuals

  Individual researchers can use networks to better understand their research outputs and plan new research efforts. For example, simply visualizing a network of outputs could already prove a useful tool for researchers to view relationships between their outputs from a different, more coherent perspective. Researchers looking for new research opportunities could also use network analysis to identify their strengths, by comparing whether specific sets of outputs are more central than others in a larger network. For example, a researcher who writes software for their research might find that their software is more central in a larger network than their findings, which could indicate a fruitful specialization. 

  One scenario where network evaluation tools could be valuable for individual researchers is in order to optimize resource allocation. A researcher might want to revisit previous work and conduct a large replication, but only has funds for one replication. Imagine a researcher who wants to identify an effect previously studied that has been central to their new research efforts. Identifying which effect to replicate is intended by this researcher as a safeguard mechanism to prevent further investment in new studies, if a fundamental finding proves to not be replicable.

  In this scenario, the researcher aims to identify the most central finding in a network. The researcher has conducted many studies throughout their career and does not want to identify the most central finding in the entire network of outputs over the years, but only of the most recent domain they've been working in. As such, the researcher takes the latest output and traces all the preceding outputs automatically, to create a subset of the full network and to incorporate potential work not done by themselves. 

  Subsequently, by computing the Katz centrality of the resulting subnetwork, the researcher can compute the number of outputs generated by a finding and how many outputs those outputs generated in return. By assigning this value to each node in the network, the researcher can identify the most central nodes. However, these nodes need to be investigated subsequently in order to see whether they are findings or something else (e.g., theory; we assume an agnostic infrastructure that does not classify nodes).

  Centrality can be a useful measure to identify which finding to replicate, but would fail to take into account what replications have already been conducted. When taking the most recent output and looking at its parent(s), grandparent(s), etc., this only looks at the lineage of the finding. However, the children of all these parents are not taken into account in such a trace. As such, the researcher in our scenario might identify an important piece of research to replicate, but neglect that it has already been replicated. Without further inspection of the network for already available replications, resource allocation might be suboptimal after all.

#5. Discussion

  We propose to communicate research in modular "as-you-go" outputs (e.g., theory followed by hypotheses, etc.) instead of large "after-the-fact" papers. Modular communication opens up the possibility of a network of knowledge to come into existence when these pieces are linked (e.g., results descend from data). This network of knowledge would be supplementary to traditional citation networks and could facilitate new evaluation tools that are based in the question of interest than generic "one size fits all" evaluation indicators (e.g., Journal Impact Factor, citation counts, number of publications). Given the countless questions and operationalizations possible to evaluate research in a network of knowledge, we hope this would increase the focus on indicators as a tool in the evaluation process instead of indicators being the evaluation process itself [@Hicks2015]. 

  We highlighted a few use cases and potential indicators for funders, research collectives, and individuals, but recognize that we are merely scratching the surface of possible use cases and implementations of network analysis in research evaluation. The use cases presented for the various target groups (e.g., universities) can readily be transferred to suit other target groups (e.g., individuals). Award committees might use critical path analysis or network stability analysis to identify key hubs in a network to recognize. Moreover, services could be built to harness the information available in a network to identify people who could be approached for collaborations or to facilitate the ease with which such network analyses can be conducted. Future work could investigate more use cases, qualitatively identify what researchers (or others) would like to know from such networks, and how existing network analysis methods could be harnessed to evaluate research and better understand its development over time. Despite our enthusiasm for network based evaluation, we also recognize the need for exploring the potential negative sides of it. Proximity effects might increase bias towards people already embedded in a network and might exacerbate inequalities already present.

  Communicating scholarly research in modular "as-you-go" outputs might also address other threats to research sustainability. In modular "as-you-go" communication, selective publication based on results would be reduced because data would be communicated before results are known. Similarly, adjusting predictions after results are known would be reduced because predictions would be communicated before data are available (i.e., preregistration by design). Replications (or reanalyses) would be encouraged both for the replicated (the replicated node gets more child nodes, increasing its centrality) and the replicator (time investment is lower due to only having to add a data node that is linked to the materials node of the replicated). Self-plagiarism could be reduced by not forcing researchers to rehash the same theory across papers that spawn various predictions and studies. These various issues (amongst other out of scope issues) could be addressed jointly instead of each issue vying for importance for researchers, funders, or policy makers (amongst others). 

  To encourage culture- and behavioral change, "after-the-fact" papers and modular "as-you-go" outputs could co-exist (initially) and would not require researchers to make a zero-sum decision. Copyright is often transferred to publishers upon publication (resulting in pay-to-access), but only after a legal contract is signed. Hence, preprints cannot be legally restricted by publishers when they precede a copyright transfer agreement. However, preprints face institutional and social opposition [@Kaiser2017], where preprinting could exclude a manuscript for publication depending on editorial policies or due to fears of non-publication or scooping (itself a result of hypercompetition). In recent years, preprints have become more widely accepted and less likely to exclude manuscript publication (e.g., Science accepts preprinted manuscripts) [@Berg2017]. Similarly, sharing modular "as-you-go" outputs would not be legally restricted by publishers and could ride the wave of preprint acceptance, but might also face institutional or social counterchange similar to the history of preprints. Researchers could communicate "as-they-go" and compile "after-the-fact" papers, facilitating co-existence and minimizing negative effects on career opportunities. Additionally, “as-you-go” modules could be used in scholarly fields where the provenance of information is important to findings.

  As far as we know, modular "as-you-go" scholarly communication infrastructure that includes direct links has not yet been available to researchers in a sustainable way. The only thought style that has facilitated "as-you-go" reporting in the past decade is that of Open Notebook Science (ONS) [@Bradley2007], where researchers share their day-to-day notes and thoughts. However, ONS has remained on the fringes of the Open Science thought style and has not matured, limiting its usefulness and uptake. For example, ONS increases user control because communication occurs on personal domains, but does not have a mechanism of preserving the content. Considering reference rot occurs in seven out of ten scholarly papers containing Weblinks [@Klein2014], concern for sustainable ONS is warranted without further development of content integrity. Moreover, ONS increases information output without providing more possibilities of discovering that content. Nanopublications have been implemented recently [@10.7717/peerj-cs.78].

  Digital infrastructure that facilitates "as-you-go" scholarly communication is now feasible and sustainable. Feasible because the peer-to-peer protocol Dat provides stable addresses for versioned content and it ensures content integrity across those versions. Sustainable because preservation in a peer-to-peer network is relatively trivial (inherent redundancy, anyone can rehost information and libraries could be persistent hosters) and removes (or at least reduces) the need for centralized services in scholarly communication. Consequently, this decreases the need for inefficient server farms of centralized services [@Cavdar2012] by decentralizing services. However, preservation is a social process that requires commitment. Hence, a peer-to-peer infrastructure would require committed and persistent peers (e.g., libraries) to make sure content is preserved. Another form of sustainability is knowledge inclusion, which is facilitated by a decentralized network protocol that is openly accessible.

  Finally, we would like to note that communication was not instantly revolutionized by the printing press but changed society over the centuries that followed. The Web has only been around since 1991 and its effect on society is already pervasive, but far from over. We hope that individuals who want change do not despair by feelings of inertia in scholarly communication throughout recent years and further entrenching of positions and interests. We remain optimistic for substantial change to occur within scholarly communication that improves the way we communicate research and hope these ideas contribute in working towards that. 

#Conclusion

  The current scholarly communication system based on research papers is “after-the-fact” and can be supplemented by an “as-you-go” based communication system. By doing so, the functions of a scholarly communication system can be interpreted more widely, making registration complete, certification part of the process instead of just the judgment of peers, access to everything for everyone based on peer-to-peer protocols, simplify archival, and facilitate incentive structures that could align researcher’s interests with that of scholarly research.

#References